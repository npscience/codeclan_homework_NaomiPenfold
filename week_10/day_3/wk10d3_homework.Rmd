---
title: 'wk10d3 homework: manual model development'
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---


# MVP

You are given a set of data on housing sale prices for the last few years in King County (near Seattle) between May 2014 and May 2015.

<br>
<div class="emphasis">
We want you to build an **explanatory model** for the `price` of housing in King County, i.e. an interpretable model in which the included variables are statistically justifiable.

The variable definitions are:

`id` - Unique ID for each home sold  
`date` - Date of the home sale  
`price` - Price of each home sold  
`bedrooms` - Number of bedrooms  
`bathrooms` - Number of bathrooms, where .5 accounts for a room with a toilet but no shower  
`sqft_living` - Square footage of the apartments interior living space  
`sqft_lot` - Square footage of the land space  
`floors` - Number of floors  
`waterfront` - A dummy variable for whether the apartment was overlooking the waterfront or not  
`view` - An index from 0 to 4 of how good the view of the property was  
`condition` - An index from 1 to 5 on the condition of the apartment  
`grade` - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design  
`sqft_above` - The square footage of the interior housing space that is above ground level  
`sqft_basement` - The square footage of the interior housing space that is below ground level  
`yr_built` - The year the house was initially built  
`yr_renovated` - The year of the houseâ€™s last renovation  
`zipcode` - What zipcode area the house is in  
`lat` - Latitude  
`long` - Longitude  
`sqft_living15` - The square footage of interior housing living space for the nearest 15 neighbors
`sqft_lot15` - The square footage of the land lots of the nearest 15 neighbors  
</div>
<br>


```{r}
library(tidyverse)
library(mosaic)
library(GGally)
library(ggfortify)
library(modelr)

kc_houses <- read_csv("data/kc_house_data.csv")
```


# Question 1

Tidy up the data ready for regression:

    * You might like to think about removing some or all of `date`, `id`, `sqft_living15`, `sqft_lot15` and `zipcode` (`lat` and `long` provide a better measure of location in any event).
    * Have a think about how to treat `waterfront`. Should we convert its type?
    * We converted `yr_renovated` into a `renovated` logical variable, indicating whether the property had ever been renovated. You may wish to do the same.
    * Have a think about how to treat `view`, `condition` and `grade`? Are they interval or categorical ordinal data types?
    
```{r}
kc_houses %>% 
  summarise(count = n(), .by = waterfront)
# no need to convert type, 163 are waterfront (1)), 21,450 are not (0)
# repeat for view, condition and condition: 
## all are effectively ordinal, ok to leave as numeric
## maybe convert condition to binned values (e.g. 1-4 poor, 5-8 ok, 9-13 good) if needed
## for now leave as is

kc_houses_trim <- kc_houses %>% 
    # renovation / year_built --> derive new var, time since last renovation
    # (time since built if never renovated)
  mutate(yrs_since_renov = if_else(
    yr_renovated > 0, 
    year(date) - yr_renovated,
    year(date) - yr_built)
  ) %>% 
    # drop unneeded vars
  select(-c(sqft_living15, sqft_lot15, zipcode, date, id))

kc_houses_trim
```

17 vars
21,613


Variable reduction: probably only need one of yr_built and yrs_since_renov, and maybe condition -- look at colinearity and pick the ones with the strongest correlation with price.

# Question 2

Check for aliased variables using the `alias()` function (this takes in a formula object and a data set). [**Hint** - formula `price ~ .` says 'price varying with all predictors', this is a suitable input to `alias()`]. Remove variables that lead to an alias. Check the 'Elements of multiple regression' lesson for a dropdown containing further information on finding aliased variables in a dataset.

```{r}
alias(price ~ ., kc_houses_trim)
```

Suggests sqft_living and sqft_above alias with sqft_basement - just use sqft_basement and drop the other two?

```{r}
kc_houses_trim <- kc_houses_trim %>% 
  select(-c(sqft_above, sqft_living))
```

```{r}
alias(price ~ ., kc_houses_trim)
```

No other vars are aliased.

# Question 3

> Systematically build a regression model containing up to **four** main effects (remember, a main effect is just a single predictor with coefficient), testing the regression diagnostics as you go. 
> Splitting datasets into numeric and non-numeric columns might help `ggpairs()` run in manageable time, although you will need to add either a `price` or `resid` column to the non-numeric dataframe in order to see its correlations with the non-numeric predictors.
> and the same in subsequent rounds of predictor selection with the `resid` column. Remember, if you are not sure whether including a categorical predictor is statistically justified, run an `anova()` test passing in the models with- and without the categorical predictor and check the p-value of the test.

```{r, message = FALSE}
houses_numeric_1 <- kc_houses_trim %>%
  select_if(is.numeric) %>% 
  select(1:8)

houses_numeric_2 <- kc_houses_trim %>%
  select_if(is.numeric) %>% 
  select(1,9:15)

ggpairs(houses_numeric_1)
ggpairs(houses_numeric_2)
```

Promising numeric predictors:

1. Number of bathrooms, number of bedrooms
2. Grade, latitude


```{r, message = FALSE}
houses_nonnumeric <- kc_houses_trim %>%
  select_if(function(x) !is.numeric(x))
# no non-numeric vars to look at
```

From corr coefficients, start with grade and num_bathrooms:

```{r}
mod1a <- lm(price ~ grade, kc_houses_trim)
mod1b <- lm(price ~ bathrooms, kc_houses_trim)
```

Check model diagnostics:

```{r}
autoplot(mod1a)
autoplot(mod1b)
```

Grade relationship does not look linear, residuals not normally distributed, and looks heteroskedastic

Latitude model is a bit better, but still not looking great. We need another variable.

```{r}
summary(mod1a)
summary(mod1b)
```

Grade has better stats (R2 is higher, residual std error is lower).

Let's try using both and compare to the one-predictor models

```{r}
mod1ab <- lm(price ~ grade + bathrooms, kc_houses_trim)

autoplot(mod1ab)
```

similar autoplot to above - a wedge shape in LHS plots, and also residuals not normally distributed at upper extreme. not a good enough model yet.

```{r}
summary(mod1ab)
```

Residual std error is less and R2 is more than both mod1a and mod1b; mod1a is next best.

Let's anova to test is mod1ab is better than mod1a:

```{r}
anova(mod1a, mod1ab)
```

Yes it is sig different and it slightly reduces RSS, so is a better model.

Alternative approach: let's try grade with a different second predictor:

```{r}
mod2a <- lm(price ~ grade + lat, kc_houses_trim)

autoplot(mod2a)
```

Looks similar to the others: still an uneven shape in the left-hand graphs, and not normally distributed at the upper extreme values.

```{r}
summary(mod2a)
```

Lower residual std error and higher R2 than mod1ab; mod2a may be a better model.

Let's compare 1a to 2a by anova (to see effect of adding lat) - can't compare grade + bathrooms to grade + lat, different models:

```{r}
anova(mod1a, mod2a)
```

Adding latitude to grade is significantly different to just having grade and also lowers the residuals sum of squares (RSS), so mod2a is also better than mod1a.

The R2 is higher, and residual std error lower, for mod2a and mod1ab, so let's use grade + lat as our first two predictors.

Let's assess the remaining predictors correlations with the remaining residuals:

```{r}
kc_houses_mod2_resid <- kc_houses_trim %>% 
  add_residuals(mod2a) %>% 
  # remove vars in our model
  select(-c(price, grade, lat))

# split df into 2
kc_houses_mod2_resid1 <- kc_houses_mod2_resid %>% 
  select(resid, 1:6)
kc_houses_mod2_resid2 <- kc_houses_mod2_resid %>% 
  select(resid, 7:12)
```

```{r, message = FALSE}
ggpairs(kc_houses_mod2_resid1)
ggpairs(kc_houses_mod2_resid2)
```

From this, more promising predictors: view, waterfront, sqft_basement, yrs_since_renovation (as well as bathrooms)

So, current model (mod2a) has price ~ grade + lat

Let's try: 
Mod3a: grade + lat + bathrooms
Mod3b: grade + lat + view

```{r}
mod3a <- lm(price ~ grade + lat + bathrooms, kc_houses_trim)
mod3b <- lm(price ~ grade + lat + view, kc_houses_trim)
```

```{r, message = FALSE}
autoplot(mod3a)
autoplot(mod3b)
```

Both seem non-linear, heterskedastic, and also residuals not normally distributed...

```{r}
summary(mod3a)
summary(mod3b)
```

Of the two, mod3b has lower residual std dev and higher R2 than mod3a:

244000
0.5584

These are better than mod2a:

259700
0.4995

Let's test with ANOVA

```{r}
anova(mod2a, mod3b)
```

mod3b is sig different and has lower RSS than mod2a so is a better model.

Best model so far: mod3b: price ~ grade + lat + view

Fourth predictor: look at residuals and the remaining predictors

```{r}
kc_houses_trim_resids3 <- kc_houses_trim %>% 
  add_residuals(mod3b) %>% 
  select(-c(price, grade, lat, view))

kc_houses_trim_resid3a <- kc_houses_trim_resids3 %>% 
  select(resid, 1:6)

kc_houses_trim_resid3b <- kc_houses_trim_resids3 %>% 
  select(resid, 7:11)
```

```{r, message = FALSE}
ggpairs(kc_houses_trim_resid3a)
ggpairs(kc_houses_trim_resid3b)
```

in order of correlation coefficient (magnitude): yr_built, sqft_basement, yrs_since renovation, waterfront, condition, bathrooms

Model already accounts for: price ~ grade + lat + view -- so how well-built, and two elements of location

note that 
yr_built and yrs_since_renovation are similar - also a reflection of condition and grade
view and waterfront are similar
bedrooms is indicator of size, as is sqft_basement and bathrooms

for fourth predictors, let's try yr_built (as age factor) and sqft_basement as indicator of size (rather than bedrooms, bathrooms)

mod4a: price ~ grade + lat + view + yr_built
mod4b: price ~ grade + lat + view + sqft_basement

```{r}
mod4a <- lm(price ~ grade + lat + view + yr_built, kc_houses_trim)
mod4b <- lm(price ~ grade + lat + view + sqft_basement, kc_houses_trim)
```

```{r}
autoplot(mod4a)
autoplot(mod4b)
```

similar comments to previous diagnostics (shape, not normal dist of residuals)... mod4b looks a bit better than mod4a

```{r}
summary(mod4a)
summary(mod4b)
```

By R2 and residual std dev, mod4a is better: 234700, 0.5913

And looks favourable to mod3b: 244000, 0.5584

```{r}
anova(mod3b, mod4a)
```

mod4a is sig different and better (lower RSS) than mod3b.

So for a four-predictor (no interactions) model of price, we have:

(mod4a) price ~ grade + lat + view + yr_built

formula:

> price = -2.057e+07 + 2.145e+05 * grade + 5.189e+05 * lat + 1.016e+05 * view -2.658e+03 * yr_built

these coefficients are awful, but it might be because yr_built is a number in range 19XXs to now, and not an age...

```{r}
kc_houses_trim <- kc_houses_trim %>% 
  mutate(age = year(today()) - yr_built)
```

```{r}
mod4a1 <- lm(price ~ grade + lat + view + age, kc_houses_trim)
```

```{r, message = FALSE}
autoplot(mod4a1)
```

```{r}
summary(mod4a1)
```

Check:

```{r}
anova(mod3b, mod4a1)
```

This hasn't changed the coefficients!! So, the formula is:

> price = -2.057e+07 + 2.145e+05 * grade + 5.189e+05 * lat + 1.016e+05 * view -2.658e+03 * yr_built

The formula means that house price is affected as follows:

* for the worst house: grade 1, no views (view = 0), and built very early on (earliest yr_built is 1900) in the lat with the lowest price (47.2638): 

> price = -880,514

This does not make actual sense - but as a baseline:

* for every 1 unit of grade improvement, the house price increases by USD 214,500
* for every 1 unit of latitude increased, the house price increases by USD 518,900
* for every 1 year more recently that the house was built, the house price decreases by USD 2,658 (which doesn't make true-world sense, but ok)
* and if the house has a view, the price is USD 101,600 higher

So the highest house prices are location more north, with a higher grade of building standard, and with a view. More recently built houses have lower prices in this particular model (but it is a much smaller effect size than the others).

Looking at the best situ

max lat = 47.7776
max grade = 13
max view = 1 (TRUE)
(max yr_built = 2015, or use 1900 again)

> price = -2.057e+07 + 2.145e+05 * 13 + 5.189e+05 * 47.7776 + 1.016e+05 * 1 -2.658e+03 * 1990

= USD 1,822,477

Which falls far short of the max price in the actual dataset

```{r}
max(kc_houses_trim$price)
```

Overall this model explains ~60% of the variation in the dataset.

Other sensible predictors might be bathrooms or bedrooms

# Extensions - not done

* Consider possible interactions between your four main effect predictors and test their effect upon $r^2$. Choose your best candidate interaction and visualise its effect. 

* Calculate the relative importance of predictors from your best $4$-predictor model (i.e. the model without an interaction). Which predictor affects `price` most strongly?

```{r}
library(relaimpo)
calc.relimp(mod4a, type = "lmg", rela = TRUE)
```

grade is the stronger relative contributor to the model
view is better than lat, perhaps should have been added as second predictor
yr_built is not that useful

consider whether grade + bathrooms + view might be better...

# homework review


* treatment of variables - note they have mutated grade, etc, to factor - dummy variable issue.
* order of attack for predictors

> Main lesson here: spend time feature engineering at the beginning (meaning get the data into the best shape for linear regression through transformation)

## working through

1. what does 1 row represent? 1 individual house
2. understand what variables there are, ideas of potential predictors
3. 

```{r}
library(tidyverse)
library(mosaic)
library(GGally)
library(ggfortify)
library(modelr)
library(skimr)

kc_houses <- read_csv("data/kc_house_data.csv")
```

```{r}
skim(kc_houses)
```

3. Feature engineering: take our raw materials, make them more valuable

(i) Consider transforming vars - any extreme values? consider the impact of some really high values when the bulk are lower (positively, right skewed, hump on left) - try a log(x) transformation to make it more normally distributed

We might want to log transform:

* price
* bedrooms
* sqft_x

(ii) Convert the ordinal columns to <fctr> to prevent the dummy variable problem.

These are:

* grade
* view
* condition

Consider grade, is a grade 13, 13 x as good as a grade 1? No - having it as a numeric does not make sense. Instead, these are ordered levels (ordinal) not numbers:

```{r}
kc_houses %>% 
  ggplot(aes(x = as.factor(grade), y = price)) +
  geom_boxplot()
```
```{r}
summary(lm(price ~ grade, data = kc_houses_trim))
summary(lm(price ~ as.factor(grade), data = kc_houses_trim))
```

Model with factored grade is better (higher R2, lower residual std errors) - but not lots of lines here, several not significant. If we have a real-world reason to, we could group the grades into bins to reduce the levels.

Note data dictionary: "An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design"

And looking at boxplot, we could consider grouping.

(iv) And convert 0/1s to logical:

* waterfront

(v) Variable and dimensionality reduction

* Drop any vars that are already not useful
* Check for aliases (columns directly/partially explained by others)
* Consider combining some vars if together they tell particular story

```{r, eval = FALSE}
houses_trim <- houses_trim %>% 
  # drop aliases: sqft_living, sqft_above
  select(-c(sqft_living, sqft_above)) %>% 
  # log transform
  mutate(log_price = log(price),
         log_sqft_basement = log(sqft_basement),
         log_sqft_lot = log(sqft_lot)) %>%
  # convert ordinal to fctr
  mutate(view = as.factor(view),
         condition = as.factor(condition)) %>% 
  # convert grade to groups and factorise
  mutate(grade_bin = case_when(
    grade %in% c(1:4) ~ 1, # low grade
    grade %in% c(5:9) ~ 2, # medium grade
    grade %in% c(10:13) ~ 3, # high grade
    .default = NA_integer_
    ),
    grade_bin = as.factor(grade_bin)) %>% 
  # convert waterfront to logical
  mutate(waterfront = as.logical(waterfront)) %>% 
  # remove replaced vars
  select(-c(price, sqft_lot, grade)) %>% 
  # put log_price as col 1 (for ggpairs top row)
  relocate(log_price, .before = bedrooms)
```

There's something odd with log_sqft_basement... log(0) --> -Inf. First need to change 0s to NAs

```{r}
houses_trim %>% 
  select(sqft_basement,log_sqft_basement) %>% 
  head()
```

Check price and sqft_lot for 0s too

```{r}
kc_houses %>% 
  filter(price == 0)

kc_houses %>% 
  filter(sqft_lot == 0)
```

Both empty so no 0s to worry about there

```{r}
# first drop the useless vars
houses_trim <- kc_houses %>%
  select(-c(zipcode, id, sqft_living15, sqft_lot15))
  
# check for aliases to drop
alias(price ~ ., houses_trim)

# now we can prep our data
houses_trim <- houses_trim %>% 
  # drop aliases: sqft_living, sqft_above
  select(-c(sqft_living, sqft_above)) %>% 
  # first recode 0s in sqft_basement
  mutate(sqft_basement = if_else(
    sqft_basement == 0,
    NA_integer_, sqft_basement)) %>% 
  # log transform
  mutate(log_price = log(price),
         log_sqft_basement = log(sqft_basement),
         log_sqft_lot = log(sqft_lot)) %>%
  # convert grade to groups and factorise
  mutate(grade_bin = case_when(
    grade %in% c(1:4) ~ 1, # low grade
    grade %in% c(5:9) ~ 2, # medium grade
    grade %in% c(10:11) ~ 3, # high grade
    grade %in% c(12:13) ~ 4, # very high grade
    .default = NA_integer_
    ),
    grade_bin = as.factor(grade_bin)) %>% 
    # convert ordinal to fctr
  mutate(view = as.factor(view),
         condition = as.factor(condition),
         grade = as.factor(grade)) %>% 
  # convert waterfront to logical
  mutate(waterfront = as.logical(waterfront)) %>% 
  # create logical for renovated or not
  mutate(renovated = if_else(
    yr_renovated == 0,
    FALSE, TRUE
  )) %>% 
  # create yrs since renovation and age of house as additional vars too
  mutate(yrs_since_renovated = if_else(
    renovated == TRUE,
    year(date) - yr_renovated,
    NA_integer_
  ),
  age_house = year(date) - yr_built) %>% 
  # remove replaced vars (keep in grade to test grade groups or not)
  select(-c(price, sqft_basement, sqft_lot, date, yr_renovated, yr_built)) %>% 
  # put log_price as col 1 (for ggpairs top row)
  relocate(log_price, .before = bedrooms)
```
```{r}
skim(houses_trim)
```


Now we have 15 potential predictor vars

Quick check for missing values

```{r}
houses_trim %>% 
  summarise(across(everything(),
                   ~ sum(is.na(.x))))
```

Only missing values are where sqft_basement was 0, and where not renovated.

```{r}
alias(log_price ~ ., houses_trim)
```

Alias picks up grade v grade_bin, but that's ok, we're interesting in testing which is better approach

Also picks up renovatedTRUE v intercept - not meaningful?

```{r}
colnames(houses_trim)
```

Ideas about predictors:

* some kind of condition: age, whether renovated or not, how ling since renovated, condition, grade_bin v grade (binned better or worse than individual grades in model?)
* some kind of size: number of floors, bathroooms, bedrooms, loq_sqft_x and whether it has a basement or not
* some kind of feature: view, waterfront 
* location: lat, long

### use ggpairs to look at corr

use ggpairs, look at absolute value of corr coeff along the log_price row

ggpairs: need to split to reduce the ggpairs down to make this easier to work with, note col 11 is log_price

```{r, warning = FALSE}
# 16 vars
houses_trim1 <- houses_trim %>% 
  select(1:6)

houses_trim2 <- houses_trim %>% 
  select(1,7:11)

houses_trim3 <- houses_trim %>% 
  select(1,12:16)

ggpairs(houses_trim1, progress = F)
# to enable looking, could save each plot (saves last one run)
ggsave("images/ggpairs1.png", width = 15, height = 15)

ggpairs(houses_trim2, progress = F)
ggsave("images/ggpairs2.png", width = 15, height = 15)

ggpairs(houses_trim3, progress = F)
ggsave("images/ggpairs3.png", width = 15, height = 15)
```

in order of absolute corr:

* **bathrooms** 0.55
* **lat** 0.45
* bedrooms 0.34
* log_sqft_basement 0.33
* floors 0.311

ordinals: ordered by size of effect from visual of boxplot

* **grade** - v obvious correlation
* **grade_bin** - v large shift across bins
* **waterfront**
* condition
* view
* renovated - v small shift

From these, ~5 variables of interest to start with, and Q of whether to use grades or grade_binned

### start building model

pick vars with high corr

add one by one (stepwise) and do ANOVA to understand whether new model with added predictor is better than previous

also see summary() and consider whether to treat the predictor differently, e.g. individual grades --> lots of lines, lots of arms in the model, and not all significant. Maybe worth grouping the grades into bins (if makes real-world sense, and looking at which grades are similar enough from the boxplots of raw data).


#### first predictors

start with bathrooms, grade and grade_bin, see which gives best first model

test bathrooms + grade v bathrooms + grade_bin to see which type of grade data to use

```{r}
model1a <- lm(log_price ~ bathrooms, houses_trim)
model1b <- lm(log_price ~ grade, houses_trim)
model1c <- lm(log_price ~ grade_bin, houses_trim)
```


### diagnostics

look at diagnostics... what can you tell about our model's predictions (fitted values) from these?

use autoplot(model) or plot(model) - shows individual plots rather than a 2x2 of plots.

* Residuals v Fitted: if we see patterns, it may be telling us we're over/under-estimating in certain ways --> the model is not ready
* Normal Q-Q: usually see values coming off the diagonal at low and high quantiles - useful to look at whether this error is symmetrical or not, otherwise indicates distribution of residuals is skewed in some way, in which case we might want to log transform a var somewhere
* Scale-Location: scale of our error in our model (size of residuals) along the axis of scale of our fitted values. We want to be wrong by the same amount across our model, so if the line or pattern here is not a 0-line, we fail on the assumption of homoskedasticity: we have unequal variance in our residuals across our fitted values.

No real answer to what is good enough to accept / bad enough to reject a model. If looking bad, maybe linear regression is not the way to model this data.

```{r}
autoplot(model1a)
autoplot(model1b)
autoplot(model1c)
```

All three look pretty good! Maybe some uneven shape in the grade models, some uneven prediction across the range of house prices

```{r}
summary(model1a)
summary(model1b)
summary(model1c)
```

Here, the best model (highest R2, lowest residual std dev) is model1b - using raw grades (as factors), not binned

    lm(formula = log_price ~ grade, data = houses_trim)
    Residual standard error: 0.3735 on 21601 degrees of freedom
    Multiple R-squared:  0.4973,	Adjusted R-squared:  0.4971 
    F-statistic:  1943 on 11 and 21601 DF,  p-value: < 2.2e-16


### add residuals to df 

to find which of the remaining predictors vars could help explain the remaining residuals

do this repeatedly as build model, to sense the next best predictors to try adding

```{r, warning = FALSE}
model1bresid <- houses_trim %>% 
  add_residuals(model1b) %>% 
  select(-c(log_price, grade, grade_bin))

model1bresid1 <- model1bresid %>% 
  select(resid, 1:7)

model1bresid2 <- model1bresid %>% 
  select(resid, 8:13)

ggpairs(model1bresid1, progress = F)
ggpairs (model1bresid2, progress = F)
```

So lat, age or waterfront might be good predictors to try adding to our model...


> Main lesson here: spend time feature engineering at the beginning (meaning get the data into the best shape for linear regression through transformation)

# my own thoughts

how to draw planes of best fit? see https://rpubs.com/ericroh/334817

