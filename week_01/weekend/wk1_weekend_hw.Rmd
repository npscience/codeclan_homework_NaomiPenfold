---
title: "wk1 weekend homework"
output: html_notebook
---

Task: explore Goodreads dataset `books.csv`.
1. Set up, read in data
2. Explore
3. Check for missing values
4. Clean up, e.g. column names
5. Insights
(6. Review notebook for good practices)


```{r set_up}
# load packages and read in data
library(tidyverse)
books <- read_csv("data/books.csv")
```
# Explore the data

```{r}
view(books)
glimpse(books)
```


```{r}
summary(books) # from pre-course work
```

Dimensions: 13 columns (variables) x 11,123 rows (observations).
According to the columns, the data includes:
- `title`: book title
- `authors`: book author or authors
- `isbn` and `isbn13`: two variants of standard unique identifier for books
- `language_code`: language it's written in, using standardised code
- `num_pages`: number of pages, i.e. a measure of the size of the book
- `publication_date`: 
publication date and publisher, as well as some rating information (average rating, number of [numeric] ratings, number of text reviews).

## Data quality and cleaning
Strategy, given time: only clean what I need to to support analysis.

Reviewing the view/summary/glimpse outputs, some potential things to clean up:
- Rename bookID and rowid using janitor - for good practice (but note, not likely to use these rows for anytihng, not informative)
- remove book_id and row_id after cleaning, before analysis because won't need them
- Reorder to put the three rating columns together? (not really needed)
- Check column type:
  - Publication_date col is character type - change to date to use?
  - isbn and isbn13 are character but contains numbers, but isbn is a unique ID so it doesn't matter that it's not numeric. Maybe suggests there are some character values in the column though, missing data, such as "unknown"

### Basic cleaning steps
```{r}
# improve naming of columns for tidyverse snake_case style

books <- books %>% 
  janitor::clean_names() %>% # changed bookID to book_id but not rowid
  rename("row_id" = "rowid")
```


```{r}
# Check for duplicate book IDs, in case repeated rows

books %>% 
  # distinct(bookID) # 11,123 rows == original data so no duplicates by bookID
  count(book_id) %>%
  arrange(desc(book_id)) # the topmost one is 1, so nothing > 1
  # filter(n > 1) # (as alternative way to look) --> shows 0 rows, so no frequency greater than 1 

```
### Check for missing values
```{r}
# Check for standard NAs

books %>% 
  summarise(across(
    .cols = where(is.numeric),
    .fns = ~sum(is.na(.x))
  )
  )

```
No NAs but from summary above, 0s in numeric columns: average_rating, num_pages, ratings_count, text_reviews_count - maybe non-standard NAs, in particular for num_pages since that would indicate a book with no pages (the others could be meaningful 0s).
```{r}
# explore 0s in num_pages

zero_pages <- books %>% 
  arrange(num_pages) %>% 
  filter(num_pages == 0)

nrow(zero_pages)
# View(zero_pages) # use to find example to spot check later.
# Note: two rows in zero_pages say 'NOT A BOOK' in author column. 
# Book with 0 to check is NA after next step: book_id 6953, title "Like Water for Chocolate"
# Note: this looks like an audiobook version (see publisher column).
```
there are 76 rows with 0 pages. This is likely to be missing data, so recode as NA.
```{r}
# recode 0s in num_pages as NAs

books <- books %>%
  mutate(num_pages = na_if(num_pages, 0))

sum(is.na(books$num_pages)) # there's now 76 NAs
# books %>% filter(book_id == 6953) # another spot check: this book now has NA num_pages

```

```{r}
# check for non-zero but low page numbers (e.g. 1-50 pages)

short_books <- books %>% 
  filter(num_pages <= 50) %>% # no more 0s so no lower bound needed
  arrange(num_pages)

nrow(short_books)

```
there are 492 rows with 1-50 pages -- these may be real, so can't automatically discount or recode.

## Evaluate which variables are clean enough to analyse this weekend
Some questions this data might offer insights on:
- Number of books published each year and average ratings
- Which books have most and least text ratings, in which languages
- Which publishers publish the most books (and which publishers publish the highest rated books)
- Length of book versus rating
- Most prolific authors (number of books per author); best rated authors

*What is possible with the data we have here (and the timeframe)?* Look at the columns and see which data is clean enough (or reasonable to clean in short time) and useful to analyse.

### Book titles
Assume these are mostly unique, but see from scanning there are some duplicate or inconsistent values, e.g. lots of "Anna Karenina" books (duplicate titles), with slightly different author combinations - this is the same book (maybe different author combos due to translated versions), but note other duplicates may be different books (different authors altogether), so cannot assume all duplicates are erroneous. 

How many duplicated titles?

```{r}
# is the number of distinct book titles less than the overall number of rows in the original dataset?

titles <- books %>% 
  distinct(title) # 10,348 rows < 11,123 rows in original

duplicates_in_titles <- nrow(books) - nrow(titles)
duplicates_in_titles

```
There are 775 duplicated book titles in this dataset.

There are also some titles that could be the same book but written inconsistently, e.g. "The Ultimate Hitchhiker's Guide: Five Complete Novels and One Story (Hitchhiker's Guide to the Galaxy  #1-5)" and "The Ultimate Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy  #1-5)".

Maybe too much potential cleaning here to work with `title` in this homework.


### Publishers
```{r}

# is the publisher data clean enough to analyse?

# find unique publisher values, order alphabetically, to help spot where same publisher entered inconsistently

 publishers_all <- books %>% 
  distinct(publisher) %>% 
  arrange(publisher)

# number of unique publisher values
nrow(publishers_all)

```
2,290 publishers (so some publishers listed more than once, have published multiple books - as expected) - but by scanning, several could be the same publisher but entered inconsistently (see Houghton Mifflin, Scholastic, Alfred A..., Amherst, etc.

Maybe too much potential cleaning here to work with `publisher` in this homework.

### Authors
```{r authors}
# is the author data clean enough to analyse?

books %>% 
  distinct(authors) %>% 
  arrange(authors)

```
6,639 rows, so some authors are listed multiple times (multiple books, as expected) - but also there are values with sometimes several authors, separated with "/". 

Maybe too much potential cleaning here to work with `authors` in this homework, but could look at *how many individual versus how many collaborative authorships in this dataset (contains("/"))?*

### Languages
```{r}
# is the language data clean enough to analyse?

 books %>% 
  distinct(language_code) %>% 
  arrange(language_code)
```
27 rows is a manageable number to clean and look at. From scanning, no obvious cleaning steps needed, no NAs or duplicates. Maybe "enm" is a typo for eng, but searching online: "enm" is for "English, middle ages", so not a typo.

### Derivative data
What additional variables could I add by processing the available data?
- Recode rating as buckets (e.g. rating >4 = highly rated, rating >2.5 = fair, rest = poorly rated )
- Age of book: to compare against number of ratings and reviews accrued over time (and look at average ratings by age, or some category of age (recoding data by buckets of age). To get age of book, would need to calculate time between today and publication date, if publication date values can be recognised as dates in R. Search online --> base R function as.Date() converts from character to date class (new to me, maybe leave for this homework). If proceed, note I might need to tell it to expect mm/dd/yyyy format. Help function suggests syntax to look for format "%m/%d/%Y": 
```
as.Date(x, format, tryFormats = c("%Y-%m-%d", "%Y/%m/%d"),
        optional = FALSE, ...)
```

# Analysis

## Analysis planning
Choosing which analyses to do.

Insight 1: the info above about data quality and the potential need to clean up authors and publishers before can do meaningful analyses with these variables.

Ideas for 4 more insights to generate using dplyr and other functions we've learned this week (and avoiding the columns where data needs more than basic cleaning, as above):

* Top and bottom books by ratings: top/bottom rated books (by number rating)
* Multi-authorship: How many individual versus collaborative authorships in this dataset (where collaborative contains("/") in authors)?
* Look at language of the books: number of books in each language, average length and average_rating for each language, top and bottom rated books for a specified language; *might want to group all english variants (eng, en-uk, en-us, etc); might want to write a new csv with language data, if useful to work on more*


4. look at what length book might be best to write/publish - recode book length to categories, and use summarise to do a count and average rating by size bucket; look at spread of book length to decide on buckets; make sure any 0-length books are encoded as NAs, check for other v small and large books
5. Number of books published each year, average length and average ratings / number of ratings by age of book (if time to recode publication date (as date, or at least extract the year)

## First, get rid of book_id and row_id since not using further

```{r}
books <- books %>% 
  select(-row_id, -book_id)
```


## Analysis: Top & bottom rated books
top/bottom books by ratings: top/bottom rated books (by number rating), books with most/least text ratings.

Note: slice_min initially returned 25 rows, all 0 average rating. This could be real but note almost all have 0 ratings count too, so are unrated. So I have first filtered the data to turn average_rating to -99 if ratings_count is 0. Note: I tried to replace `average_rating` value with `NA_character_` if `ratings_count` is 0, but error: could not combine a <chr> for TRUE with <dbl> for FALSE when using if_else.
```{r ratings_cleaning_before}
# impute average_ratings to NA if ratings_count is 0 - step 1: check what we expect to change.

books %>% 
  filter(ratings_count == 0) # 80 rows with 0s.
  
sum(is.na(books$average_rating)) # 0 NAs initially in average_ratings

```

```{r ratings_cleaning_after}
# impute average_ratings to -99 if ratings_count is 0, so can filter this later - step 2: change it.

books <- books %>% 
  mutate(average_rating_clean = if_else(
            ratings_count == 0, -99, average_rating)) 

books %>% 
  filter(average_rating_clean < 0) # see 80 rows of -99 in new column

# Note: old code that was wrong:
# mutate(average_rating = na_if(ratings_count, 0)) 
# this overwrote average_rating with ratings_count value for all (including NA for 0)
```


```{r}
top_books <- books %>% 
  slice_max(average_rating_clean, n=10) %>% 
  arrange(title)  # make alphabetical
top_books

top_books_titles <- top_books %>% 
  select(title) %>%
  pull()
top_books_titles
```

There are 19 top-rated books here (not top 10) because they all have an equal average rating of 5.

Note: in previous try using average_rating, before cleaning the ratings with rating_count of 0 to -99, there were 22 top books. So the cleaning step helped.

```{r}
# find books with lowest average rating

bottom_books <- books %>% 
  filter(ratings_count > 0) %>% 
  slice_min(average_rating_clean, n = 10) %>% 
  arrange((desc(average_rating_clean))) # number 10 is the lowest rated
bottom_books

bottom_books_titles <- bottom_books %>% 
  select(title) %>%
  pull()
bottom_books_titles
```

The 10 lowest rated books have average ratings of 1.00 to 2.40.

## Analysis: multi-authorship
From above (see code chunk `authors`), we know there are 6,639 distinct values in `authors`, without any cleaning steps.

Some look like one author, others contain multiple authors, separated by /. So we can look at ratio single : multiple authorship using the logical test for whether it contains "/" - but not yet covered this kind of logical test (if a character is within a string) - google suggests `grepl()` but we've not covered this yet, so leaving this idea for now.

```{r, eval = FALSE}

# Not working. 

# books %>% 
#   mutate(num_of_authors = 
#            if_else(authors %in% "/", "multiple", "single"),
#          .after = authors
#         )

```

## Analysis: by language
Task: Look at language of the books: number of books in each language, average length and average_rating for each language, top and bottom rated books for a specified language.

```{r}
# Find number of books by language

books_per_language <- books %>% # assign to new df to reuse later
  group_by(language_code) %>% 
  summarise(num_of_books = n()) %>%  # Number of books in each language
  arrange(desc(num_of_books))

books_per_language
```

There are 27 different languages (as previously found), or 23 if we count all types of English as one. Most books listed in Goodreads dataset are written in English (general, American and British), with ~100s in Spanish, French and German.


```{r}
# Summarise averages of ratings count and text reviews count by language (to see how much attention different language books get on Goodreads)

books_activity_by_language <- books %>%
  group_by(language_code) %>% 
  summarise(across(
    .cols = c(ratings_count, text_reviews_count),
    .fns = mean # don't need to remove NAs because none in these cols, so not using lambda function
  )) %>% 
  arrange(desc(ratings_count))

books_activity_by_language
```

Books written in English (general) receive the most ratings and text reviews on the Goodreads platform, although several other languages get 1000s of numeric reviews too. Books written in languages other than English seem to get fewer text reviews, though =  Swedish books have the most text reviews out of all non-English books (157, the only language with >100 text reviews). 
